{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a786b1",
   "metadata": {},
   "source": [
    "WEBSCRAPING ASSIGNMENT - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c9ee0a",
   "metadata": {},
   "source": [
    "1) Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup                                             # import required modules\n",
    "import requests                                                       \n",
    "\n",
    "\n",
    "def header_tags(url):                                                     # defining the function with url as parameter\n",
    "    \n",
    "    page = requests.get(url)                                              # get URL\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')                     # scrape webpage\n",
    "    heading_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])    # scrape required data\n",
    "    for i in heading_tags:             \n",
    "        print(i,\"\\n\")                                                     # display scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e26fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_tags('https://en.wikipedia.org/wiki/Main_Page')                    # calling the function with url as a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c4ea4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup                                             # import required modules\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def top_100(url):                                                         # defining a function with url as parameter\n",
    "    page = requests.get(url)                                              # get URL\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')                     # scrape webpage\n",
    "\n",
    "    scraped_movies = soup.find_all(\"td\", class_ = \"titleColumn\")          # scrape required data\n",
    "    Title = []\n",
    "    for i in scraped_movies:\n",
    "        i = i.get_text().replace('\\n', '').strip()\n",
    "        i = i.rstrip(i[-6:])\n",
    "        Title.append(i)\n",
    "    Title = Title[:100]\n",
    "\n",
    "    scraped_year = soup.find_all(\"span\", class_ = \"secondaryInfo\")\n",
    "    Year = []\n",
    "    for i in scraped_year:\n",
    "        i = i.get_text().replace('(','').replace(')', '')\n",
    "        Year.append(i)\n",
    "    Year = Year[:100]\n",
    "    \n",
    "    scraped_rating = soup.find_all(\"td\", class_ = \"ratingColumn imdbRating\")\n",
    "    Rating = []\n",
    "    for i in scraped_rating:\n",
    "        i = i.get_text().replace('\\n','')\n",
    "        Rating.append(i)\n",
    "    Rating = Rating[:100] \n",
    "\n",
    "    for n in range(0,100):\n",
    "        print(Title[n],', Rating : ',Rating[n],' Year : ', Year[n])        # display scrapped data\n",
    "\n",
    "    Top_100 = pd.DataFrame({\"Movie Name\" : Title, \"Rating\" : Rating, \"Year of release\" : Year}) # dataframe of scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a60dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100('https://www.imdb.com/chart/top/')                                 # calling the function with url as a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2263864",
   "metadata": {},
   "source": [
    "3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdfa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup                                              # import required modules\n",
    "import requests \n",
    "import pandas as pd\n",
    "\n",
    "def top_100_indian(url):                                                   # defining a function with url as parameter\n",
    "    page = requests.get(url)                                               # get URL\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')                      # scrape webpage\n",
    "\n",
    "    scraped_movies = soup.find_all(\"td\", class_ = \"titleColumn\")           # scrape required data\n",
    "    Title = []\n",
    "    for i in scraped_movies:\n",
    "        i = i.get_text().replace('\\n', '').strip()\n",
    "        i = i.rstrip(i[-6:])\n",
    "        Title.append(i)\n",
    "    Title = Title[:100]\n",
    "\n",
    "    scraped_year = soup.find_all(\"span\", class_ = \"secondaryInfo\")\n",
    "    Year = []\n",
    "    for i in scraped_year:\n",
    "        i = i.get_text().replace('(','').replace(')', '')\n",
    "        Year.append(i)\n",
    "    Year = Year[:100]\n",
    "    \n",
    "    scraped_rating = soup.find_all(\"td\", class_ = \"ratingColumn imdbRating\")\n",
    "    Rating = []\n",
    "    for i in scraped_rating:\n",
    "        i = i.get_text().replace('\\n','')\n",
    "        Rating.append(i)\n",
    "    Rating = Rating[:100] \n",
    "\n",
    "    for n in range(0,100):\n",
    "        print(Title[n],', Rating : ',Rating[n],', Year : ', Year[n])                            # display scrapped data\n",
    "\n",
    "    Top_100 = pd.DataFrame({\"Movie Name\" : Title, \"Rating\" : Rating, \"Year of release\" : Year}) # dataframe of scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3666e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_indian('https://www.imdb.com/india/top-rated-indian-movies/')           # calling the function with url as a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596df89",
   "metadata": {},
   "source": [
    "4) Write a python program to scrape product name, price and discounts from https://meesho.com/bags-ladies/pl/p7vbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def meesho_prod(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    prod_name = soup.find_all(\"div\", class_ = \"Card__BaseCard-sc-b3n78k-0 cXRroa NewProductCard__DetailCard_Desktop-sc-j0e7tu-2 eEBPAI NewProductCard__DetailCard_Desktop-sc-j0e7tu-2 eEBPAI\")\n",
    "    Name = []\n",
    "    for i in prod_name:\n",
    "        i = i.get_text()\n",
    "        head, sep, tail = i.partition('₹')\n",
    "        Name.append(head)\n",
    "\n",
    "    prod_price = soup.find_all(\"div\", class_ = \"Card__BaseCard-sc-b3n78k-0 fVRkfg NewProductCard__PriceRow-sc-j0e7tu-5 dYYUrF NewProductCard__PriceRow-sc-j0e7tu-5 dYYUrF\")\n",
    "    Selling_price = []\n",
    "    for i in prod_price:\n",
    "        i = i.get_text()\n",
    "        head,sep,tail = i.partition('₹')\n",
    "        i=tail\n",
    "        head,sep,tail = i.partition('₹')\n",
    "        Selling_price.append(head)\n",
    "\n",
    "    Off = []\n",
    "    for i in prod_name:\n",
    "        i = i.get_text()\n",
    "        head,sep,tail = i.partition('off')\n",
    "        i=head[-4:]+'off already applied'\n",
    "        Off.append(i)\n",
    "    \n",
    "    Discount = []\n",
    "    for i in prod_name:\n",
    "        i = i.get_text()\n",
    "        head,sep,tail = i.partition('off')\n",
    "        i=tail\n",
    "        head,sep,tail = i.partition('order')\n",
    "        i=head + 'order'\n",
    "        Discount.append(i)\n",
    "    \n",
    "    return(pd.DataFrame({'Product_name' : Name, 'Selling Price' : Selling_price, 'Off already applied over MRP': Off, 'Discount' : Discount }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meesho_prod('https://meesho.com/bags-ladies/pl/p7vbp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102c131",
   "metadata": {},
   "source": [
    "5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def top_teams_men(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    team = soup.find_all('span', class_ = 'u-hide-phablet')\n",
    "    Team = []\n",
    "    for i in team:\n",
    "        i = i.get_text()\n",
    "        Team.append(i)\n",
    "    Team = Team[:10]\n",
    "\n",
    "    Top_team_m = soup.find('td', class_ = 'rankings-block__banner--matches').get_text()\n",
    "    Top_team_p = soup.find('td', class_ = 'rankings-block__banner--points').get_text()\n",
    "    Top_team_r = soup.find('td', class_ = 'rankings-block__banner--rating u-text-right').get_text().replace('\\n','').replace(' ', '')\n",
    "\n",
    "    match_point = soup.find_all('td', class_ = 'table-body__cell u-center-text')\n",
    "    Match_Point = []\n",
    "    for i in match_point:\n",
    "        i = i.get_text()\n",
    "        Match_Point.append(i)\n",
    "\n",
    "    Match = [Top_team_m] \n",
    "    Point = [Top_team_p]\n",
    "    i=0  \n",
    "    j=1\n",
    "    while i<17:\n",
    "        Match.append(Match_Point[i])\n",
    "        Point.append(Match_Point[j])\n",
    "        i = i+2 \n",
    "        j= j+2\n",
    "\n",
    "    rating = soup.find_all('td', class_ = 'table-body__cell u-text-right rating')\n",
    "    Rating = [Top_team_r]\n",
    "    for i in rating:\n",
    "        i = i.get_text()\n",
    "        Rating.append(i)\n",
    "    Rating = Rating[:10]\n",
    "\n",
    "    return(pd.DataFrame({'Team':Team, 'No. of Matches':Match, 'Points':Point, 'Rating':Rating}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_teams_men('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae516c",
   "metadata": {},
   "source": [
    "b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def top_odi_batsmen(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "    name = soup.find_all('td', class_ = 'table-body__cell rankings-table__name name')\n",
    "    Top_player = soup.find('div', class_ = 'rankings-block__banner--name-large').get_text()\n",
    "    Name = [Top_player]\n",
    "    for i in name:\n",
    "        i = i.get_text().replace('\\n','')\n",
    "        Name.append(i)\n",
    "    Name = Name[:10]\n",
    "\n",
    "    team = soup.find_all('span', class_ = 'table-body__logo-text')\n",
    "    Top_player_team = soup.find('div', class_ = 'rankings-block__banner--nationality').get_text().replace('\\n','')\n",
    "    Team = [Top_player_team]\n",
    "    for i in team:\n",
    "        i = i.get_text().replace('\\n','')\n",
    "        Team.append(i)\n",
    "    Team = Team[:10]\n",
    "\n",
    "    rating = soup.find_all('td', class_ = 'table-body__cell rating')\n",
    "    Top_player_rating = soup.find('div', class_ = 'rankings-block__banner--rating').get_text()\n",
    "    Rating = [Top_player_rating]\n",
    "    for i in rating:\n",
    "        i = i.get_text()\n",
    "        Rating.append(i)\n",
    "    Rating = Rating[:10]\n",
    "\n",
    "    return(pd.DataFrame({'Name':Name, 'Team':Team, 'Rating':Rating}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_odi_batsmen('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6d753",
   "metadata": {},
   "source": [
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def top_odi_bowler(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    page = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "    name = soup.find_all('td', class_ = 'table-body__cell rankings-table__name name')\n",
    "    Top_player = soup.find('div', class_ = 'rankings-block__banner--name-large').get_text()\n",
    "    Name = [Top_player]\n",
    "    for i in name:\n",
    "        i = i.get_text().replace('\\n','')\n",
    "        Name.append(i)\n",
    "    Name = Name[:10]\n",
    "\n",
    "    team = soup.find_all('span', class_ = 'table-body__logo-text')\n",
    "    Top_player_team = soup.find('div', class_ = 'rankings-block__banner--nationality').get_text().replace('\\n','')\n",
    "    Team = [Top_player_team]\n",
    "    for i in team:\n",
    "        i = i.get_text().replace('\\n','')\n",
    "        Team.append(i)\n",
    "    Team = Team[:10]\n",
    "\n",
    "    rating = soup.find_all('td', class_ = 'table-body__cell rating')\n",
    "    Top_player_rating = soup.find('div', class_ = 'rankings-block__banner--rating').get_text()\n",
    "    Rating = [Top_player_rating]\n",
    "    for i in rating:\n",
    "        i = i.get_text()\n",
    "        Rating.append(i)\n",
    "    Rating = Rating[:10]\n",
    "\n",
    "    return(pd.DataFrame({'Name':Name, 'Team':Team, 'Rating':Rating}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e07bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_odi_bowler('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38f0ee",
   "metadata": {},
   "source": [
    "6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b95152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def top_teams_women(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    team = soup.find_all('span', class_ = 'u-hide-phablet')\n",
    "    Team = []\n",
    "    for i in team:\n",
    "        i = i.get_text()\n",
    "        Team.append(i)\n",
    "    Team = Team[:10]\n",
    "\n",
    "    Top_team_m = soup.find('td', class_ = 'rankings-block__banner--matches').get_text()\n",
    "    Top_team_p = soup.find('td', class_ = 'rankings-block__banner--points').get_text()\n",
    "    Top_team_r = soup.find('td', class_ = 'rankings-block__banner--rating u-text-right').get_text().replace('\\n','').replace(' ', '')\n",
    "\n",
    "    match_point = soup.find_all('td', class_ = 'table-body__cell u-center-text')\n",
    "    Match_Point = []\n",
    "    for i in match_point:\n",
    "        i = i.get_text()\n",
    "        Match_Point.append(i)\n",
    "\n",
    "    Match = [Top_team_m] \n",
    "    Point = [Top_team_p]\n",
    "    i=0  \n",
    "    j=1\n",
    "    while i<17:\n",
    "        Match.append(Match_Point[i])\n",
    "        Point.append(Match_Point[j])\n",
    "        i = i+2 \n",
    "        j= j+2\n",
    "\n",
    "    rating = soup.find_all('td', class_ = 'table-body__cell u-text-right rating')\n",
    "    Rating = [Top_team_r]\n",
    "    for i in rating:\n",
    "        i = i.get_text()\n",
    "        Rating.append(i)\n",
    "    Rating = Rating[:10]\n",
    "\n",
    "    return(pd.DataFrame({'Team':Team, 'No. of Matches':Match, 'Points':Point, 'Rating':Rating}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_teams_women('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0a388",
   "metadata": {},
   "source": [
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e064f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def top_odi_batting_woman(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "    name = soup.find_all('td', class_ = 'table-body__cell rankings-table__name name')\n",
    "    Top_player = soup.find('div', class_ = 'rankings-block__banner--name-large').get_text()\n",
    "    Name = [Top_player]\n",
    "    for i in name:\n",
    "        i = i.get_text().replace('\\n','')\n",
    "        Name.append(i)\n",
    "    Name = Name[:10]\n",
    "\n",
    "    team = soup.find_all('span', class_ = 'table-body__logo-text')\n",
    "    Top_player_team = soup.find('div', class_ = 'rankings-block__banner--nationality').get_text().replace('\\n','')\n",
    "    Team = [Top_player_team]\n",
    "    for i in team:\n",
    "        i = i.get_text().replace('\\n','')\n",
    "        Team.append(i)\n",
    "    Team = Team[:10]\n",
    "\n",
    "    rating = soup.find_all('td', class_ = 'table-body__cell rating')\n",
    "    Top_player_rating = soup.find('div', class_ = 'rankings-block__banner--rating').get_text()\n",
    "    Rating = [Top_player_rating]\n",
    "    for i in rating:\n",
    "        i = i.get_text()\n",
    "        Rating.append(i)\n",
    "    Rating = Rating[:10]\n",
    "\n",
    "    return(pd.DataFrame({'Name':Name, 'Team':Team, 'Rating':Rating}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5737dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_odi_batting_woman('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb92a7f",
   "metadata": {},
   "source": [
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4eebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def top_odi_allrounder_woman(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "    name = soup.find_all('td', class_ = 'table-body__cell rankings-table__name name')\n",
    "    Top_player = soup.find('div', class_ = 'rankings-block__banner--name-large').get_text()\n",
    "    Name = [Top_player]\n",
    "    for i in name:\n",
    "        i = i.get_text().replace('\\n','')\n",
    "        Name.append(i)\n",
    "    Name = Name[:10]\n",
    "\n",
    "    team = soup.find_all('span', class_ = 'table-body__logo-text')\n",
    "    Top_player_team = soup.find('div', class_ = 'rankings-block__banner--nationality').get_text().replace('\\n','')\n",
    "    Team = [Top_player_team]\n",
    "    for i in team:\n",
    "        i = i.get_text().replace('\\n','')\n",
    "        Team.append(i)\n",
    "    Team = Team[:10]\n",
    "\n",
    "    rating = soup.find_all('td', class_ = 'table-body__cell rating')\n",
    "    Top_player_rating = soup.find('div', class_ = 'rankings-block__banner--rating').get_text()\n",
    "    Rating = [Top_player_rating]\n",
    "    for i in rating:\n",
    "        i = i.get_text()\n",
    "        Rating.append(i)\n",
    "    Rating = Rating[:10]\n",
    "\n",
    "    return(pd.DataFrame({'Name':Name, 'Team':Team, 'Rating':Rating}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_odi_allrounder_woman('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70032322",
   "metadata": {},
   "source": [
    "#Sample code to scrape data from multiple pages of same website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f85be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "Heading = []\n",
    "for j in range(1,18):                                              # j counts number of pages in that website\n",
    "    page = requests.get('https://coreyms.com/page/'+str(j))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    for i in soup.find_all('a', class_= 'entry-title-link'):\n",
    "        Heading.append(i.get_text())\n",
    "Heading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56dac6",
   "metadata": {},
   "source": [
    "7) Write a python program to scrape details of all the posts from coreyms.com. Scrape the heading, date, content\n",
    "and the code for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def coreyms_data(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "    Heading = []\n",
    "    Content = []\n",
    "    Date = []\n",
    "    Video_url = []\n",
    "    for article in soup.find_all('article'):\n",
    "        Heading.append(article.find('a', class_= 'entry-title-link').text)\n",
    "        Content.append(article.find('div', class_= 'entry-content').p.text)\n",
    "        Date.append(article.find('time', class_= 'entry-time').text)\n",
    "        try:\n",
    "            vid_src = article.find('iframe', class_='youtube-player')['src']\n",
    "            vid_id = vid_src.split('/')[4].split('?')[0]\n",
    "            Video_url.append(f'https://youtube.com/watch?v={vid_id}')\n",
    "        except Exception as e:\n",
    "            Video_url.append('N/A')\n",
    "\n",
    "    return(pd.DataFrame({'Heading':Heading, 'Date':Date, 'Content':Content, 'Video_link':Video_url}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coreyms_data('http://coreyms.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683cf236",
   "metadata": {},
   "source": [
    "8) Write a python program to scrape house details from mentioned URL. It should include house title, location, area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, Rajaji Nagar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def house_data(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    Title = []\n",
    "    for i in soup.find_all('span', class_ = 'overflow-hidden overflow-ellipsis whitespace-nowrap max-w-80pe po:max-w-full'):\n",
    "        Title.append(i.text)\n",
    "\n",
    "    Location = []\n",
    "    for i in soup.find_all('div', class_ = 'mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0 po:max-w-95'):\n",
    "        Location.append(i.text)\n",
    "\n",
    "    data = [] \n",
    "    for i in soup.find_all('div', class_ = 'p-1.5p flex border-b border-b-solid border-cardbordercolor tp:py-1p tp:px-1.5p tp:border-b-0'):\n",
    "        data.append(i.text.split('₹'))\n",
    "    Area = []\n",
    "    EMI = []\n",
    "    Price = []\n",
    "    Price_per_sqft = []\n",
    "    for i in data:\n",
    "        Area.append(i[1].replace('Builtup', ''))\n",
    "        EMI.append('₹ '+i[2].replace('Estimated EMI',''))\n",
    "        Price.append('₹ '+i[3])\n",
    "        Price_per_sqft.append('₹ '+i[4])\n",
    "\n",
    "    return(pd.DataFrame({'Title':Title, 'Location':Location, 'Builtup Area':Area, 'Estimated EMI per month':EMI, 'Price':Price, 'Price per sq ft':Price_per_sqft}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d30d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data('https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciIsInNob3dNYXAiOmZhbHNlfSx7ImxhdCI6MTIuOTMwNzczNSwibG9uIjo3Ny41ODM4MzAyLCJwbGFjZUlkIjoiQ2hJSjJkZGxaNWdWcmpzUmgxQk9BYWYtb3JzIiwicGxhY2VOYW1lIjoiSmF5YW5hZ2FyIiwic2hvd01hcCI6ZmFsc2V9LHsibGF0IjoxMi45OTgxNzMyLCJsb24iOjc3LjU1MzA0NDU5OTk5OTk5LCJwbGFjZUlkIjoiQ2hJSnhmVzREUE05cmpzUktzTlRHLTVwX1FRIiwicGxhY2VOYW1lIjoiUmFqYWppbmFnYXIiLCJzaG93TWFwIjpmYWxzZX1d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31b28d",
   "metadata": {},
   "source": [
    "9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def restuarant(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    Restuarant_name = []\n",
    "    Cuisine = []\n",
    "    Location = []\n",
    "    for i in soup.find_all('div', class_ = 'restnt-info cursor'):\n",
    "        Restuarant_name.append(i.a.text)\n",
    "        Location.append(i.div.text)\n",
    "    for i in soup.find_all('span', class_ = 'double-line-ellipsis'):\n",
    "        head, sep, tail = i.text.partition('|')\n",
    "        Cuisine.append(tail)\n",
    "\n",
    "    Image_url = []\n",
    "    for i in soup.find_all('img', class_ = 'no-img'):\n",
    "        Image_url.append(i['data-src'])\n",
    "\n",
    "    Rating = []\n",
    "    for i in soup.find_all('div', class_ = 'restnt-rating rating-4'):\n",
    "        Rating.append(i.text)\n",
    "\n",
    "    return(pd.DataFrame({'Restuarant_name':Restuarant_name, 'Cuisine':Cuisine, 'Location':Location, 'Rating':Rating, 'Image_url':Image_url}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "restuarant('https://www.dineout.co.in/delhi-restaurants/welcome-back')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566ed50",
   "metadata": {},
   "source": [
    "10) Write a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/women-tshirts?ga_q=tshirts ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df28c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def women_tshirts(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "    data = []\n",
    "    for i in soup.find_all('div', class_ = 'productCardDetail'):\n",
    "        data.append(i.text.split('₹'))\n",
    "\n",
    "    Product_name = []\n",
    "    Price = []    \n",
    "    for i in data[:10]:\n",
    "        Product_name.append(i[0])\n",
    "        Price.append('₹ '+str(int(int(i[1].replace('FEW LEFT', ''))/1000)))\n",
    "\n",
    "    Image_url = []\n",
    "    for i in soup.find_all('div', class_='productImg'):\n",
    "        Image_url.append(i.img['src'])\n",
    "        Image_url = Image_url[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ffa207b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Them Half Sleeve T-shirt</td>\n",
       "      <td>₹ 349</td>\n",
       "      <td>https://images.bewakoof.com/t320/not-them-half...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women's Pink Candy Cotton Tie &amp; Dye Boyfriend ...</td>\n",
       "      <td>₹ 499</td>\n",
       "      <td>https://images.bewakoof.com/t320/women-s-pink-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I Want too Half Sleeve T-shirt</td>\n",
       "      <td>₹ 349</td>\n",
       "      <td>https://images.bewakoof.com/t320/i-want-too-ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women's Royal Blue Cotton Tie &amp; Dye Boyfriend ...</td>\n",
       "      <td>₹ 499</td>\n",
       "      <td>https://images.bewakoof.com/t320/women-s-royal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy High Half Sleeve T-shirt</td>\n",
       "      <td>₹ 349</td>\n",
       "      <td>https://images.bewakoof.com/t320/happy-high-ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Women's Sea Green Cotton Ombre Tie &amp; Dye T-Shirt</td>\n",
       "      <td>₹ 399</td>\n",
       "      <td>https://images.bewakoof.com/t320/women-s-sea-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Women's Pink Cotton Tie &amp; Dye T-Shirt</td>\n",
       "      <td>₹ 399</td>\n",
       "      <td>https://images.bewakoof.com/t320/women-s-pink-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Happy High Boyfriend T-shirt</td>\n",
       "      <td>₹ 349</td>\n",
       "      <td>https://images.bewakoof.com/t320/happy-high-bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Coca Cola Classic Color Block Short Top</td>\n",
       "      <td>₹ 499</td>\n",
       "      <td>https://images.bewakoof.com/t320/coca-cola-blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Coca Cola Color Block Short Top</td>\n",
       "      <td>₹ 449</td>\n",
       "      <td>https://images.bewakoof.com/t320/coca-cola-blu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product_name  Price  \\\n",
       "0                       Not Them Half Sleeve T-shirt  ₹ 349   \n",
       "1  Women's Pink Candy Cotton Tie & Dye Boyfriend ...  ₹ 499   \n",
       "2                     I Want too Half Sleeve T-shirt  ₹ 349   \n",
       "3  Women's Royal Blue Cotton Tie & Dye Boyfriend ...  ₹ 499   \n",
       "4                     Happy High Half Sleeve T-shirt  ₹ 349   \n",
       "5   Women's Sea Green Cotton Ombre Tie & Dye T-Shirt  ₹ 399   \n",
       "6              Women's Pink Cotton Tie & Dye T-Shirt  ₹ 399   \n",
       "7                       Happy High Boyfriend T-shirt  ₹ 349   \n",
       "8            Coca Cola Classic Color Block Short Top  ₹ 499   \n",
       "9                    Coca Cola Color Block Short Top  ₹ 449   \n",
       "\n",
       "                                           Image_url  \n",
       "0  https://images.bewakoof.com/t320/not-them-half...  \n",
       "1  https://images.bewakoof.com/t320/women-s-pink-...  \n",
       "2  https://images.bewakoof.com/t320/i-want-too-ha...  \n",
       "3  https://images.bewakoof.com/t320/women-s-royal...  \n",
       "4  https://images.bewakoof.com/t320/happy-high-ha...  \n",
       "5  https://images.bewakoof.com/t320/women-s-sea-g...  \n",
       "6  https://images.bewakoof.com/t320/women-s-pink-...  \n",
       "7  https://images.bewakoof.com/t320/happy-high-bo...  \n",
       "8  https://images.bewakoof.com/t320/coca-cola-blo...  \n",
       "9  https://images.bewakoof.com/t320/coca-cola-blu...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "women_tshirts('https://www.bewakoof.com/women-t-shirts')\n",
    "\n",
    "pd.DataFrame({'Product_name':Product_name, 'Price':Price, 'Image_url':Image_url})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
