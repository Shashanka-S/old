{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef2027b",
   "metadata": {},
   "source": [
    "WEBSCRAPING ASSIGNMENT - 2 (using selenium)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb23c7",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "1 normal method and 1 method by defining function shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce042083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import required libraries\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.get('https://www.naukri.com')\n",
    "#calling webdriver and getting into website\n",
    "    \n",
    "desig = driver.find_element_by_class_name(\"suggestor-input\")\n",
    "desig.send_keys('Data Analyst')\n",
    "#Entering required input in job search field\n",
    "    \n",
    "loc = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")\n",
    "loc.send_keys('Bangalore')\n",
    "#Entering required input in location search field\n",
    "    \n",
    "search = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search.click()\n",
    "#clicking search button\n",
    "\n",
    "title = []\n",
    "comp = []\n",
    "loc = []\n",
    "exp = []\n",
    "    \n",
    "title_tags = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title_tags:\n",
    "    title.append(i.text) \n",
    "\n",
    "company_tags = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company_tags:\n",
    "    comp.append(i.text) \n",
    "\n",
    "experienc_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for i in experienc_tags:\n",
    "    exp.append(i.text) \n",
    "\n",
    "location_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for i in location_tags:\n",
    "    loc.append(i.text) \n",
    "#scraping all the required data by xpath route\n",
    "\n",
    "driver.close()\n",
    "#Closing the web browser after scraping\n",
    "\n",
    "jobs = pd.DataFrame({'Job-title':title[0:10], 'Company_name':comp[0:10], 'Job-location':loc[0:10], 'Experience_required':exp[0:10]})   \n",
    "# saving data into a data frame\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5ee605c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import required libraries\n",
    "\n",
    "def job_details(url):\n",
    "#defining a function to scrape data\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    #calling webdriver and getting into website\n",
    "    \n",
    "    desig = driver.find_element_by_class_name(\"suggestor-input\")\n",
    "    desig.send_keys('Data Analyst')\n",
    "    #Entering required input in job search field\n",
    "    \n",
    "    loc = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")\n",
    "    loc.send_keys('Bangalore')\n",
    "    #Entering required input in location search field\n",
    "    \n",
    "    search = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "    search.click()\n",
    "    #clicking search button\n",
    "    \n",
    "    title = []\n",
    "    comp = []\n",
    "    loc = []\n",
    "    exp = []\n",
    "    \n",
    "    title_tags = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "    for i in title_tags:\n",
    "        title.append(i.text) \n",
    "\n",
    "    company_tags = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "    for i in company_tags:\n",
    "        comp.append(i.text) \n",
    "\n",
    "    experienc_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "    for i in experienc_tags:\n",
    "        exp.append(i.text) \n",
    "\n",
    "    location_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "    for i in location_tags:\n",
    "        loc.append(i.text) \n",
    "    #scraping all the required data by xpath route\n",
    "\n",
    "    driver.close()\n",
    "    #Closing the web browser after scraping\n",
    "\n",
    "    jobs = pd.DataFrame({'Job-title':title[0:10], 'Company_name':comp[0:10], 'Job-location':loc[0:10], 'Experience_required':exp[0:10]})   \n",
    "    # saving data into a data frame\n",
    "    return(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9681569",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details('https://www.naukri.com')\n",
    "#calling the function to scrape data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f9c04",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1067e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import required libraries\n",
    "\n",
    "def job_details(url):\n",
    "#defining a function to scrape data\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    #calling webdriver and getting into website\n",
    "    \n",
    "    desig = driver.find_element_by_class_name(\"suggestor-input\")\n",
    "    desig.send_keys('Data Scientist')\n",
    "    #Entering required input in job search field\n",
    "    \n",
    "    loc = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")\n",
    "    loc.send_keys('Bangalore')\n",
    "     #Entering required input in location search field\n",
    "        \n",
    "    search = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "    search.click()\n",
    "    #clicking search button\n",
    "    \n",
    "    title = []\n",
    "    comp = []\n",
    "    loc = []\n",
    "    \n",
    "    title_tags = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "    for i in title_tags:\n",
    "        title.append(i.text) \n",
    "\n",
    "    company_tags = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "    for i in company_tags:\n",
    "        comp.append(i.text) \n",
    "\n",
    "    location_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "    for i in location_tags:\n",
    "        loc.append(i.text) \n",
    "    #scraping all the required data by xpath route\n",
    "\n",
    "    driver.close()\n",
    "    #Closing the web browser after scraping\n",
    "\n",
    "    jobs = pd.DataFrame({'Job-title':title[0:10], 'Company_name':comp[0:10], 'Job-location':loc[0:10]})   \n",
    "    # saving data into a data frame\n",
    "    return(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33463df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details('https://www.naukri.com')\n",
    "#calling the function to scrape data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df2a8df",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b77dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import required libraries\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.naukri.com')\n",
    "#calling webdriver and getting into website\n",
    "\n",
    "desig = browser.find_element_by_class_name(\"suggestor-input\")\n",
    "desig.send_keys('Data Scientist')\n",
    "#Entering required input in job search field\n",
    "\n",
    "search = browser.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search.click()\n",
    "#clicking search button\n",
    "\n",
    "for i in range(5):\n",
    "    try:\n",
    "        box = browser.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[3]/label/p/span[1]\")\n",
    "        browser.execute_script(\"arguments[0].click();\", box)\n",
    "    except NoSuchElementException as e:\n",
    "        time.sleep(1)\n",
    "#applying location filter \"Delhi/NCR\"\n",
    "        \n",
    "for i in range(5):\n",
    "    try:\n",
    "        box = browser.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]\")\n",
    "        browser.execute_script(\"arguments[0].click();\", box)\n",
    "    except NoSuchElementException as e:\n",
    "        time.sleep(1)\n",
    "#applying salary filter \"3-6 lakhs\"\n",
    "        \n",
    "title = []\n",
    "comp = []\n",
    "loc = []\n",
    "exp = []\n",
    "    \n",
    "title_tags = browser.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title_tags:\n",
    "    title.append(i.text) \n",
    "\n",
    "company_tags = browser.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company_tags:\n",
    "    comp.append(i.text) \n",
    "\n",
    "experienc_tags = browser.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for i in experienc_tags:\n",
    "    exp.append(i.text) \n",
    "    \n",
    "location_tags = browser.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for i in location_tags:\n",
    "    loc.append(i.text)\n",
    "#scraping all the required data by xpath route\n",
    "    \n",
    "browser.close()\n",
    "#Closing the web browser after scraping\n",
    "\n",
    "pd.DataFrame({'Job-title':title[0:10], 'Company_name':comp[0:10], 'Job-location':loc[0:10], 'Experience_required':exp[0:10]})                                      \n",
    "# creating a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659ac14",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands andmore” is written and click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this pageyou can scrap the required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom ofthe page , then click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2afabc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import required libraries\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.flipkart.com')\n",
    "#calling webdriver and getting into website\n",
    "\n",
    "try:\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "except:\n",
    "    pass\n",
    "#closing login pop-up\n",
    "\n",
    "driver.find_element_by_xpath(\"//input[@name='q']\").send_keys('sunglasses')\n",
    "driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click()\n",
    "#Entering required input in product search field and clicking search button\n",
    "\n",
    "brand=[]\n",
    "prod=[]\n",
    "price=[]\n",
    "\n",
    "def flipkart_scrape():\n",
    "#defining a function to scrape data from 1 page and move to next page\n",
    "    brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    prod_tags = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price_tags = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    for i in brand_tags:\n",
    "        brand.append(i.text) \n",
    "    for i in prod_tags:\n",
    "        prod.append(i.text)\n",
    "    for i in price_tags:\n",
    "        price.append(i.text)\n",
    "    #scraping all the required data by xpath route\n",
    "    try:\n",
    "        next_page = driver.find_element_by_link_text(\"NEXT\")\n",
    "        driver.execute_script(\"arguments[0].click();\", next_page)\n",
    "        time.sleep(5)   \n",
    "    except NoSuchElementException as e:\n",
    "        time.sleep(1)    \n",
    "    #moving to next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff196cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    flipkart_scrape()\n",
    "#calling the function    \n",
    "\n",
    "pd.DataFrame({'Brand':brand[0:100], 'Product':prod[0:100], 'Price':price[0:100]})\n",
    "#creating a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919a79f",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace.\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5393ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import required libraries\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace.')\n",
    "#calling webdriver and getting into website\n",
    "\n",
    "rating = []\n",
    "review = []\n",
    "review_full = []\n",
    "\n",
    "driver.find_element_by_xpath(\"//div[@class='_3UAT2v _16PBlm']\").click()\n",
    "# clicking show all reviews\n",
    "\n",
    "def flipkart_scrape():\n",
    "#defining a function to scrape data from 1 page and move to next page\n",
    "    rating_tags = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for i in rating_tags:\n",
    "        rating.append(i.text) \n",
    "\n",
    "    review_tags = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    for i in review_tags:\n",
    "        review.append(i.text) \n",
    "\n",
    "    review_full_tags = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "    for i in review_full_tags:\n",
    "        review_full.append(i.text) \n",
    "        #scraping all the required data by xpath route\n",
    "    try:\n",
    "        next_page = driver.find_element_by_link_text(\"NEXT\")\n",
    "        driver.execute_script(\"arguments[0].click();\", next_page)\n",
    "        time.sleep(5)   \n",
    "    except NoSuchElementException as e:\n",
    "        time.sleep(1)\n",
    "        #moving to next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(11):\n",
    "    flipkart_scrape()\n",
    "#calling the function\n",
    "    \n",
    "rating = rating[0:100]\n",
    "review = review[0:100]\n",
    "review_full = review_full[0:100]\n",
    "\n",
    "pd.DataFrame({'Rating':rating, 'Review':review, 'Full review':review_full})\n",
    "#creating a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc36045",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7ef91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import required libraries\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.flipkart.com')\n",
    "#calling webdriver and getting into website\n",
    "\n",
    "try:\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "except:\n",
    "    pass\n",
    "#closing login pop-up\n",
    "\n",
    "driver.find_element_by_xpath(\"//input[@name='q']\").send_keys('sneakers')\n",
    "driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click()\n",
    "#Entering required input in product search field and clicking search button\n",
    "\n",
    "brand=[]\n",
    "prod=[]\n",
    "price=[]\n",
    "\n",
    "def flipkart_scrape():\n",
    "#defining a function to scrape data from 1 page and move to next page\n",
    "    brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    prod_tags = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price_tags = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    for i in brand_tags:\n",
    "        brand.append(i.text) \n",
    "    for i in prod_tags:\n",
    "        prod.append(i.text)\n",
    "    for i in price_tags:\n",
    "        price.append(i.text) \n",
    "    #scraping all the required data by xpath route\n",
    "    try:\n",
    "        next_page = driver.find_element_by_link_text(\"NEXT\")\n",
    "        driver.execute_script(\"arguments[0].click();\", next_page)\n",
    "        time.sleep(5)   \n",
    "    except NoSuchElementException as e:\n",
    "        time.sleep(1)  \n",
    "    #moving to next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    flipkart_scrape()\n",
    "#calling the function   \n",
    "\n",
    "pd.DataFrame({'Brand':brand[0:100], 'Product':prod[0:100], 'Price':price[0:100]})\n",
    "#creating a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c429a",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 7149 to Rs. 14099 ” , Color filter to “Black”\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "#import required libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.myntra.com/shoes')\n",
    "#calling webdriver and getting into website\n",
    "\n",
    "for i in range(5):\n",
    "    try:\n",
    "        box1 = browser.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/input')\n",
    "        browser.execute_script(\"arguments[0].click();\", box1)\n",
    "    except NoSuchElementException as e:\n",
    "        time.sleep(1)\n",
    "#Set Price filter to “Rs. 7149 to Rs. 14099” \n",
    "        \n",
    "for i in range(5):\n",
    "    try:\n",
    "        box2 = browser.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label')\n",
    "        browser.execute_script(\"arguments[0].click();\", box2)\n",
    "    except NoSuchElementException as e:\n",
    "        time.sleep(1)\n",
    "#Set Price Color filter to “Black”\n",
    "\n",
    "brand = []\n",
    "desc = [] \n",
    "price = [] \n",
    "def myntra_shoes():\n",
    "    brand_tags = browser.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "    for i in brand_tags:\n",
    "        brand.append(i.text)  \n",
    "    desc_tags = browser.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "    for i in desc_tags:\n",
    "        desc.append(i.text) \n",
    "    price_tags = browser.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "    for i in price_tags:\n",
    "        price.append(i.text) \n",
    "    time.sleep(30)\n",
    "    #scraping all the required data by xpath route\n",
    "    try:\n",
    "        next_page = browser.find_element_by_link_text(\"Next\")\n",
    "        browser.execute_script(\"arguments[0].click();\", next_page)\n",
    "    except NoSuchElementException as e:\n",
    "        time.sleep(1)\n",
    "    time.sleep(15)\n",
    "    #moving to next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    myntra_shoes()\n",
    "#calling the function  \n",
    "    \n",
    "pd.DataFrame({'Brand':brand, 'Product_description':desc, 'Price':price})\n",
    "#creating a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f296bd",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import required libraries\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.amazon.in/')\n",
    "#calling webdriver and getting into website\n",
    "\n",
    "product_search = browser.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "product_search.send_keys('Laptop')\n",
    "#Entering required input in product search field\n",
    "\n",
    "search = browser.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search.click()\n",
    "#clicking search button\n",
    "\n",
    "browser.find_element_by_xpath('//*[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]/span/a/span').click()\n",
    "#browser.find_element_by_xpath('//*[@id=\"p_n_feature_thirteen_browse-bin/16757432031\"]/span/a/span').click() #for i9 category\n",
    "#While applying i7 and i9 filter it was found that amazn website not allow simultaneous selection of both selections simultaneously\n",
    "\n",
    "title = []\n",
    "rating = []\n",
    "price = []\n",
    "\n",
    "title_tags = browser.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "for i in title_tags:\n",
    "    title.append(i.text) \n",
    "\n",
    "price_tags = browser.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "for i in price_tags:\n",
    "    price.append(i.text) \n",
    "    \n",
    "rating_tags = browser.find_elements_by_xpath('//div[@class=\"a-row a-size-small\"]/span[1]')\n",
    "for i in rating_tags:\n",
    "    rating.append(i.get_attribute('aria-label'))\n",
    "#scraping all the required data by xpath route\n",
    "\n",
    "laptops = pd.DataFrame({'Title':title[0:10], 'Price':price[0:10], 'Rating':rating[0:10]})\n",
    "#saving data into a data frame\n",
    "laptops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0212be5",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter “Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter “Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99426bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "#import required libraries\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.ambitionbox.com/')\n",
    "#calling webdriver and getting into website\n",
    "\n",
    "job = browser.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[6]\")\n",
    "job.click()\n",
    "#clicking job button\n",
    "\n",
    "search_field = browser.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input\")\n",
    "search_field.send_keys('Data Scientist')\n",
    "#Entering required input in job search field\n",
    "\n",
    "search = browser.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button\")\n",
    "search.click()\n",
    "#clicking search button\n",
    "time.sleep(2)\n",
    "\n",
    "browser.find_element_by_xpath(\"//div[@title='Location']\").click()\n",
    "search_loc = browser.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input\")\n",
    "search_loc.send_keys('Noida')\n",
    "#Entering required input in location filter\n",
    "time.sleep(2)\n",
    "\n",
    "select_loc = browser.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label\")\n",
    "select_loc.click()\n",
    "#Clicking location\n",
    "\n",
    "title = []\n",
    "comp = []\n",
    "loc = []\n",
    "exp = []\n",
    "\n",
    "def Noida_jobs():\n",
    "#defining a function to scrape data\n",
    "    title_tags = browser.find_elements_by_xpath(\"//a[@class='title noclick']\")\n",
    "    for i in title_tags:\n",
    "        title.append(i.text) \n",
    "\n",
    "    company_tags = browser.find_elements_by_xpath(\"//p[@class='company body-medium']\")\n",
    "    for i in company_tags:\n",
    "        comp.append(i.text) \n",
    "\n",
    "    experienc_tags = browser.find_elements_by_xpath(\"//p[@class='body-small-l']\")\n",
    "    for i in experienc_tags:\n",
    "        if ('years') in i.text:\n",
    "            exp.append(i.text)\n",
    "        elif ('Yrs') in i.text:\n",
    "            exp.append(i.text) \n",
    "    #scraping all the required data by xpath route\n",
    "        \n",
    "    return(pd.DataFrame({'Job-title':title[0:10], 'Company_name':comp[0:10], 'Experience_required':exp[0:10]}))\n",
    "    # saving data into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4bf05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Noida_jobs()\n",
    "#calling the function to scrape data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0eb287",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary. The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aaeba42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "#import required libraries\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.ambitionbox.com/')\n",
    "#calling webdriver and getting into website\n",
    "\n",
    "browser.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[4]\").click()\n",
    "#clicking salaries button\n",
    "\n",
    "browser.find_element_by_id(\"jobProfileSearchbox\").send_keys(\"Data Scientist\")\n",
    "browser.find_element_by_id(\"jobProfileSearchbox\").click()\n",
    "#Entering required input in job search field\n",
    "time.sleep(5)\n",
    "\n",
    "comp = []\n",
    "record = []\n",
    "exp = []\n",
    "sal = []\n",
    "mi = []\n",
    "ma = []\n",
    "av = []\n",
    "\n",
    "def Data_scientist():\n",
    "#defining a function to scrape data\n",
    "    comp_tags = browser.find_elements_by_xpath(\"//div[@class='name']/a\")\n",
    "    for i in comp_tags:\n",
    "        comp.append(i.text) \n",
    "\n",
    "    record_tags = browser.find_elements_by_xpath(\"//div[@class='name']/span\")\n",
    "    for i in record_tags:\n",
    "        record.append(i.text) \n",
    "\n",
    "    exp_tags = browser.find_elements_by_xpath(\"//div[@class='salaries sbold-list-header']\")\n",
    "    for i in exp_tags:\n",
    "        i = i.text\n",
    "        head, sep, tail = i.partition('\\n . \\n')\n",
    "        exp.append(tail)\n",
    "\n",
    "    sal_tags = browser.find_elements_by_xpath(\"//div[@class='salary-values']/div\")\n",
    "    for i in sal_tags:\n",
    "        sal.append(i.text) \n",
    "    salary = sal[-20:]\n",
    "    for i in range(0,20,2):\n",
    "        mi.append(salary[i])\n",
    "        i = i+1\n",
    "        ma.append(salary[i])\n",
    "\n",
    "    av_tags = browser.find_elements_by_xpath(\"//p[@class='averageCtc']\")\n",
    "    for i in av_tags:\n",
    "        av.append(i.text) \n",
    "    #scraping all the required data by xpath route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scientist()\n",
    "#calling the function to scrape data\n",
    "\n",
    "Data_scientist_salary = pd.DataFrame({'Company':comp, 'No. of salaries recorded':record, 'Work experience':exp, 'Average salary':av[-10:], 'Minimum salary':mi, 'Maximum salary':ma})\n",
    "#saving data as dataframe\n",
    "\n",
    "Data_scientist_salary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
